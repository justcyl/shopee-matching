{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "def get_y_true(df):\n",
    "    \"\"\"\n",
    "    Get true prediction indices\n",
    "    \"\"\"\n",
    "    index = Series(np.arange(df.shape[0]))\n",
    "    grp = index.groupby(df['label_group'].values).agg(list)\n",
    "\n",
    "    return df['label_group'].map(grp).tolist()\n",
    "\n",
    "def get_validation_folds(df, nfolds=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Function to create validation folds. Split not only by label group, but also by title, image, phash\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for col in ['label_group', 'title', 'image_phash', 'image']:\n",
    "\n",
    "        agg = df.groupby(col)['posting_id'].agg(list).tolist()\n",
    "        for p in agg:\n",
    "            nx.add_path(G, p)\n",
    "\n",
    "    cc = {}\n",
    "    for n, c in enumerate(nx.connected_components(G)):\n",
    "        val = min(c)\n",
    "        for x in c:\n",
    "            cc[x] = val\n",
    "\n",
    "    group_idx = df['posting_id'].map(cc).values\n",
    "    groups = np.unique(group_idx)\n",
    "    np.random.shuffle(groups)\n",
    "\n",
    "    split = np.array_split(groups, nfolds)\n",
    "\n",
    "    folds = np.zeros(df.shape[0], dtype=np.int32)\n",
    "\n",
    "    for n, s in enumerate(split):\n",
    "        folds[np.isin(group_idx, s)] = n\n",
    "\n",
    "    return folds\n",
    "\n",
    "\n",
    "data = pd.read_csv('../input/train_min.csv')\n",
    "device = 'cuda:0'\n",
    "y_true = get_y_true(data)\n",
    "folds = get_validation_folds(data, 5, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lowercase': True, 'ngram_range': (1, 1)}\n",
      "{'lowercase': True, 'ngram_range': (3, 3), 'analyzer': 'char'}\n",
      "4.938139674894642\n",
      "4.746989765201686\n",
      "CPU times: user 706 ms, sys: 307 ms, total: 1.01 s\n",
      "Wall time: 858 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import torch,gc\n",
    "def get_dist_features(D):\n",
    "    \"\"\"\n",
    "    Get density features for embed point\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for i in [2, 3, 5, 10, 20, 50]:\n",
    "        features.append(D[:, 1: i].mean(axis=1))\n",
    "\n",
    "    for i in [.5, .6, .7, .8, .9, .95, .97, .99]:\n",
    "        features.append((D >= i).sum(axis=1))\n",
    "\n",
    "    return np.stack(features, axis=1).astype(np.float32)\n",
    "def csr_to_torch_sparse(csr_mat):\n",
    "    \"\"\"\n",
    "    Transform csr matrix to torch Sparse format\n",
    "    \"\"\"\n",
    "    coo_mat = csr_mat.astype(np.float32).tocoo()\n",
    "\n",
    "    row = torch.from_numpy(coo_mat.row).type(torch.int64)\n",
    "    col = torch.from_numpy(coo_mat.col).type(torch.int64)\n",
    "    edge_index = torch.stack([row, col], dim=0)\n",
    "\n",
    "    val = torch.from_numpy(coo_mat.data)\n",
    "    out = torch.sparse.FloatTensor(edge_index, val, torch.Size(coo_mat.shape))\n",
    "\n",
    "    return out\n",
    "def get_di_torch(embed, n_candidates=50, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Calc distances/indices matrices from embeddings\n",
    "    \"\"\"\n",
    "    D = np.zeros((embed.shape[0], n_candidates), dtype=np.float32)\n",
    "    I = np.zeros((embed.shape[0], n_candidates), dtype=np.int32)\n",
    "\n",
    "    flg_dense = isinstance(embed, np.ndarray)\n",
    "\n",
    "    if flg_dense:\n",
    "        embed_cuda = torch.from_numpy(embed).cuda()\n",
    "    else:\n",
    "        embed_cuda = csr_to_torch_sparse(embed).cuda()\n",
    "\n",
    "    for i in range(0, embed.shape[0], batch_size):\n",
    "\n",
    "        if flg_dense:\n",
    "            embed_batch = embed_cuda[i: i + batch_size]\n",
    "            d = torch.matmul(embed_cuda, embed_batch.T).T\n",
    "        else:\n",
    "            embed_batch = torch.from_numpy(embed[i: i + batch_size].toarray().T).cuda()\n",
    "            d = torch.matmul(embed_cuda, embed_batch).T\n",
    "\n",
    "        idx = torch.argsort(d, dim=1, descending=True)[:, :n_candidates]\n",
    "        I[i: i + batch_size, :idx.shape[1]] = idx.cpu().numpy()\n",
    "        D[i: i + batch_size, :idx.shape[1]] = torch.gather(d, 1, idx).cpu().numpy()\n",
    "\n",
    "    del d, idx, embed_cuda, embed_batch\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return D, I\n",
    "\n",
    "def cutoff_prediction(D, I, cutoff, exact_add=2):\n",
    "    \"\"\"\n",
    "    Cutoff prediction of distances/indices matrices\n",
    "    \"\"\"\n",
    "    res = []\n",
    "\n",
    "    ranger = np.arange(D.shape[1])\n",
    "\n",
    "    for d, i in zip(D, I):\n",
    "        res.append(i[(d > cutoff) | (ranger < exact_add)])\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_tfidf_embed(data, param_list, cutoffs):\n",
    "    \"\"\"\n",
    "    Get TfIdf embeddings with different tokenize params\n",
    "    \"\"\"\n",
    "    tfidf_embed, tfidf_D, tfidf_I, tfidf_points = [], [], [], []\n",
    "\n",
    "    for params in param_list:\n",
    "        vect = TfidfVectorizer(**params, dtype=np.float32)\n",
    "        tfidf_embed.append(vect.fit_transform(data['title']))\n",
    "\n",
    "        _d, _i = get_di_torch(tfidf_embed[-1])\n",
    "        tfidf_D.append(_d)\n",
    "        tfidf_I.append(_i)\n",
    "\n",
    "        tfidf_points.append(get_dist_features(tfidf_D[-1]))\n",
    "        print(params)\n",
    "        \n",
    "    tfidf_preds = []\n",
    "\n",
    "    for d, i, co in zip(tfidf_D, tfidf_I, cutoffs):\n",
    "\n",
    "        tfidf_preds.append(cutoff_prediction(d, i, co))\n",
    "        print(sum(map(len, tfidf_preds[-1])) / len(data))\n",
    "\n",
    "    del tfidf_I, tfidf_D\n",
    "    gc.collect()\n",
    "        \n",
    "    return tfidf_embed, tfidf_preds, tfidf_points\n",
    "    \n",
    "\n",
    "tfidf_embed, tfidf_preds, tfidf_points = get_tfidf_embed(data,                  \n",
    "                                        param_list = [\n",
    "                                            {'lowercase': True, 'ngram_range': (1, 1)}, \n",
    "                                            {'lowercase': True, 'ngram_range': (3, 3),\n",
    "                                             'analyzer': 'char'},  \n",
    "                                        ], \n",
    "                                        cutoffs=[0.45, 0.45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> [<6644x9564 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 54743 stored elements in Compressed Sparse Row format>, <6644x14186 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 322282 stored elements in Compressed Sparse Row format>]\n"
     ]
    }
   ],
   "source": [
    "print(type(tfidf_embed),tfidf_embed[0:1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
