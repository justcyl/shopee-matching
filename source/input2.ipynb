{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T13:27:46.152911553Z",
     "start_time": "2023-11-05T13:27:46.135134746Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>../input/train_images/0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "      <td>train_129225211 train_2278313361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>../input/train_images/00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DOUBLE FOAM TAPE</td>\n",
       "      <td>2937985045</td>\n",
       "      <td>train_3386243561 train_3423213080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>../input/train_images/000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>2395904891</td>\n",
       "      <td>train_2288590299 train_3803689425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>../input/train_images/00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Campur - Leher Kancing (DPT001-00) Batik karakter Alhadi</td>\n",
       "      <td>4093212188</td>\n",
       "      <td>train_2406599165 train_3342059966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>../input/train_images/00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
       "      <td>3648931069</td>\n",
       "      <td>train_3369186413 train_921438619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id  \\\n",
       "0   train_129225211   \n",
       "1  train_3386243561   \n",
       "2  train_2288590299   \n",
       "3  train_2406599165   \n",
       "4  train_3369186413   \n",
       "\n",
       "                                                        image  \\\n",
       "0  ../input/train_images/0000a68812bc7e98c42888dfb1c07da0.jpg   \n",
       "1  ../input/train_images/00039780dfc94d01db8676fe789ecd05.jpg   \n",
       "2  ../input/train_images/000a190fdd715a2a36faed16e2c65df7.jpg   \n",
       "3  ../input/train_images/00117e4fc239b1b641ff08340b429633.jpg   \n",
       "4  ../input/train_images/00136d1cf4edede0203f32f05f660588.jpg   \n",
       "\n",
       "        image_phash  \\\n",
       "0  94974f937d4c2433   \n",
       "1  af3f9460c2838f0f   \n",
       "2  b94cb00ed3e50f78   \n",
       "3  8514fc58eafea283   \n",
       "4  a6f319f924ad708c   \n",
       "\n",
       "                                                                                                title  \\\n",
       "0                                                                           Paper Bag Victoria Secret   \n",
       "1                                        Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DOUBLE FOAM TAPE   \n",
       "2                                                         Maling TTS Canned Pork Luncheon Meat 397 gr   \n",
       "3  Daster Batik Lengan pendek - Motif Acak / Campur - Leher Kancing (DPT001-00) Batik karakter Alhadi   \n",
       "4                                                                   Nescafe \\xc3\\x89clair Latte 220ml   \n",
       "\n",
       "   label_group                             target  \n",
       "0    249114794   train_129225211 train_2278313361  \n",
       "1   2937985045  train_3386243561 train_3423213080  \n",
       "2   2395904891  train_2288590299 train_3803689425  \n",
       "3   4093212188  train_2406599165 train_3342059966  \n",
       "4   3648931069   train_3369186413 train_921438619  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COMPUTE_CV = True\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../input/utils\")\n",
    "import helpers\n",
    "# DATA_PATH = \"../input/shopee-product-matching/\"\n",
    "DATA_PATH = \"../input/\"\n",
    "\n",
    "\n",
    "train = pd.read_csv(DATA_PATH + \"train.csv\")\n",
    "train[\"target\"] = train.label_group.map(train.groupby(\"label_group\").posting_id.agg(\"unique\").to_dict())\n",
    "train[\"target\"] = train[\"target\"].apply(lambda x: \" \".join(x))\n",
    "train[\"image\"] = DATA_PATH + \"train_images/\" + train[\"image\"]\n",
    "ttrain=train\n",
    "if COMPUTE_CV == False:\n",
    "    train = pd.read_csv(DATA_PATH + \"test.csv\")\n",
    "    train[\"image\"] = DATA_PATH + \"test_images/\" + train[\"image\"]\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T13:28:02.188621318Z",
     "start_time": "2023-11-05T13:27:47.158656217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Model Backbone for eca_nfnet_l1 model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d045c5f9c04566910a5aa2b20b076d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4282 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our image embeddings shape is torch.Size([34250, 11014])\n"
     ]
    }
   ],
   "source": [
    "import eca_nfnet_l1_arc_face\n",
    "\n",
    "img_embs = eca_nfnet_l1_arc_face.get_test_embeddings(\n",
    "    train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T13:28:07.806344417Z",
     "start_time": "2023-11-05T13:28:02.191765916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Model Backbone for ../input/shopee-models/paraphrase-xlm-r-multilingual-v1 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get_bert_embeddings: 100%|██████████████████| 1072/1072 [00:19<00:00, 56.30it/s]\n"
     ]
    }
   ],
   "source": [
    "import xlm_v1_arcface\n",
    "\n",
    "bert_embs = xlm_v1_arcface.get_test_embeddings(\n",
    "    train,\"title\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "torch.Size([34250, 11014]) torch.Size([34250, 768])\n"
     ]
    }
   ],
   "source": [
    "print(type(img_embs),type(bert_embs))\n",
    "print(img_embs.shape,bert_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T13:28:07.819246316Z",
     "start_time": "2023-11-05T13:28:07.817527818Z"
    }
   },
   "outputs": [],
   "source": [
    "np.save(\"../input/img_embs.npy\", img_embs.cpu())\n",
    "np.save(\"../input/bert_embs.npy\", bert_embs.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embs=np.load(\"../input/img_embs.npy\")\n",
    "bert_embs=np.load(\"../input/bert_embs.npy\")\n",
    "img_embs = torch.from_numpy(img_embs)\n",
    "bert_embs = torch.from_numpy(bert_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T13:28:07.839774019Z",
     "start_time": "2023-11-05T13:28:07.821172509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34250\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img_embs=F.normalize(img_embs)\n",
    "bert_embs=F.normalize(bert_embs)\n",
    "\n",
    "set_size = len(img_embs)\n",
    "print(set_size)\n",
    "\n",
    "\n",
    "test_df = helpers.add_measurements(train)\n",
    "\n",
    "new_embs = helpers.blend_embs([img_embs, bert_embs], test_df)\n",
    "\n",
    "combined_inds, combined_dists = helpers.combined_distances(new_embs) #n*K, 返回第i个样本第j大的相似度和对应的索引\n",
    "# helpers.check_measurements(combined_dists, combined_inds, test_df)\n",
    "pairs = helpers.sorted_pairs(combined_dists, combined_inds) #对矩阵\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1746750\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# ds=[[],[]]\n",
    "# la=[]\n",
    "# for x,y,v in pairs:\n",
    "#     ds[0].append(min(1,np.dot(img_embs[x].cpu(), img_embs[y].cpu())))\n",
    "#     ds[1].append(min(1,np.dot(bert_embs[x].cpu(), bert_embs[y].cpu())))\n",
    "#     # print(x)\n",
    "#     la.append(ttrain.iloc[x].label_group==ttrain.iloc[y].label_group)\n",
    "    # print(x,y,v)\n",
    "# print(mi)\n",
    "# print(ds[0])\n",
    "# plt.scatter(ds[0],ds[1], c=la)\n",
    "# plt.xlim([0, 1.1])\n",
    "# plt.ylim([0, 1.1])\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T13:29:49.393484765Z",
     "start_time": "2023-11-05T13:29:42.670408492Z"
    }
   },
   "outputs": [],
   "source": [
    "groups = [[] for _ in range(set_size)]\n",
    "groups_p = [[] for _ in range(set_size)]\n",
    "for x,y,v in pairs:\n",
    "    groups[x].append(y)\n",
    "    groups_p[x].append(v)\n",
    "for pos, size_pct in helpers.get_targets_shape(ttrain):\n",
    "    helpers.chisel(groups, groups_p, pos, int(size_pct * len(groups))) # 一组的长度，该长度的组数\n",
    "    # print(pos, int(size_pct * len(groups)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-05T13:20:59.919434761Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>train_2278313361 train_129225211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>train_3386243561 train_3423213080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>train_2288590299 train_3803689425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>train_2406599165 train_1744956981 train_1508100548 train_3526771004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>train_921438619 train_3369186413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id  \\\n",
       "0   train_129225211   \n",
       "1  train_3386243561   \n",
       "2  train_2288590299   \n",
       "3  train_2406599165   \n",
       "4  train_3369186413   \n",
       "\n",
       "                                                               matches  \n",
       "0                                     train_2278313361 train_129225211  \n",
       "1                                    train_3386243561 train_3423213080  \n",
       "2                                    train_2288590299 train_3803689425  \n",
       "3  train_2406599165 train_1744956981 train_1508100548 train_3526771004  \n",
       "4                                     train_921438619 train_3369186413  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = [' '.join(test_df.iloc[g].posting_id.to_list()) for g in groups]\n",
    "test_df['matches'] = matches\n",
    "\n",
    "test_df[['posting_id','matches']].to_csv('submission.csv',index=False)\n",
    "pd.read_csv('submission.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-05T13:20:59.920729195Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(train['target'].head(), test_df['matches'].head())\n",
    "# import eval_preds\n",
    "# train['precision'],train['recall'],train['f1'] =  eval_preds.get_score(train['target'], test_df['matches'])\n",
    "# print(train['f1'].mean(),train['recall'].mean(),train['precision'].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
